{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Learning Objectives](#Learning-Objectives)\n",
    "* [Pandas for Reading and Writing Data](#Pandas-for-Reading-and-Writing-Data)\n",
    "\t* [Set-Up](#Set-Up)\n",
    "\t* [Example Data](#Example-Data)\n",
    "* [Demonstration](#Demonstration)\n",
    "\t* [Creating a Temporary Directory](#Creating-a-Temporary-Directory)\n",
    "\t* [Reading and Inspecting](#Reading-and-Inspecting)\n",
    "\t* [Minimal Cleanup](#Minimal-Cleanup)\n",
    "\t\t* [Listing Columns](#Listing-Columns)\n",
    "\t\t* [Extracting Columns](#Extracting-Columns)\n",
    "\t\t* [Adding Columns](#Adding-Columns)\n",
    "\t\t* [Deleting Columns](#Deleting-Columns)\n",
    "\t\t* [Renaming Columns](#Renaming-Columns)\n",
    "* [File Formats](#File-Formats)\n",
    "\t* [CSV](#CSV)\n",
    "\t* [CSV Online](#CSV-Online)\n",
    "\t* [Excel](#Excel)\n",
    "\t* [SQLite](#SQLite)\n",
    "\t* [JSON](#JSON)\n",
    "\t* [HDF](#HDF)\n",
    "\t* [FAQ: File Formats](#FAQ:-File-Formats)\n",
    "* [Reading Large Data](#Reading-Large-Data)\n",
    "\t* [Read Performance](#Read-Performance)\n",
    "\t\t* [Timing Reads: Common Formats](#Timing-Reads:-Common-Formats)\n",
    "\t\t* [Timing Reads: Pickle and MSGPack](#Timing-Reads:-Pickle-and-MSGPack)\n",
    "\t* [Reading Compressed Data](#Reading-Compressed-Data)\n",
    "\t\t* [Set-Up](#Set-Up)\n",
    "\t\t* [Read Compressed file from Disk](#Read-Compressed-file-from-Disk)\n",
    "\t\t* [Read from URL and Extract to Disk](#Read-from-URL-and-Extract-to-Disk)\n",
    "\t\t* [Read from URL and Extract Without Disk](#Read-from-URL-and-Extract-Without-Disk)\n",
    "\t* [Reading Chunks](#Reading-Chunks)\n",
    "* [Cleaning Temporary Files](#Cleaning-Temporary-Files)\n",
    "* [Section Review](#Section-Review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this notebook, the student will be able to:\n",
    "* Use pandas to read and inspect file data\n",
    "* Use pandas to read and write CSV, Excel, SQLite, and HDF files.\n",
    "* Time the read and write operations so as to profile file I/O.\n",
    "* Read larger data files, either as compressed files, or in chunks\n",
    "* Select a file format more suited for a particular task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas for Reading and Writing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas has excellent support for file I/O (read/write) for a wide variety of common file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import modules needed for this lesson\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "# Set Jupyter notebook display options\n",
    "pd.options.display.max_rows = 6\n",
    "pd.options.display.max_columns = 8\n",
    "\n",
    "# Test the version of pandas: it should be 0.16 or greater\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The data used throughout this demonstration is from the catalog of exoplanets -- planets outside our solar system -- compiled by [exoplanets.org](http://exoplanets.org/table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Temporary Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time you are handling data, it is commonlly needed to handle \"tmp\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Remove old tmp dir, Create a new tmp dir\n",
    "import os\n",
    "if not os.path.exists(\"tmp\"):\n",
    "    os.mkdir(\"tmp\")\n",
    "\n",
    "# Define a convenience function to help us clean up\n",
    "def clean_tmp(file_name=\"tmp/exoplanet.csv\"):\n",
    "    if os.path.isfile(file_name):\n",
    "        os.remove(file_name)\n",
    "    elif os.path.exists(file_name):\n",
    "        # if it is a directory, then do it recusively (you may have a __pycache__ folder there)\n",
    "        for f in os.listdir(file_name):\n",
    "            clean_tmp(os.path.join(file_name, f))\n",
    "        os.rmdir(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Inspecting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple demonstration of reading and inspecting data.\n",
    "\n",
    "We will see a much more detailed treatment of examining your data in the next lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_name = \"data/exoplanets.csv\"\n",
    "data = pd.read_csv(file_name)\n",
    "print(type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preview data after a read.\n",
    "# Notice the first column\n",
    "data.iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# There is an Index array imbedded in the DataFrame\n",
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We can use a column to create a better Index\n",
    "data = pd.read_csv(file_name, parse_dates=True, index_col='DATE')\n",
    "data.head(10)     # another way to preview data after a read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notice it is not just an Index, but a DatetimeIndex; note the dtype\n",
    "data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can sort by the Index\n",
    "data = data.set_index(data.index.sort_values(ascending=False))\n",
    "data.iloc[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, we will cover data clean-up in much more detail. \n",
    "\n",
    "Here are just a few simple things to do to inspect and clean-up data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View the labels of some or all of the columns\n",
    "# data.columns[0:10]\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract_data = data['RADIUS(rjupiter)']\n",
    "print(type(extract_data))\n",
    "extract_data.iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- zero fill: \n",
    "    ```df['var'] = 0```\n",
    "   - values from NumPy array: ```df['my_data'] = data```\n",
    "   - note: df.var construct can not create a column by that\n",
    "     name; only used to access existing columns by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add a column labeled 'price'\n",
    "data['price'] = 1e6 # One *MILLION* dollars!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the updated DataFrame\n",
    "data.price.iloc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Demonstration that you can delete columns after a file read\n",
    "del data['FIRSTURL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Inspect the results\n",
    "data.iloc[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change the label of a single column\n",
    "data = data.rename(columns={'NAME':'PLANET'})\n",
    "data.iloc[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has an extensive set of I/O methods:\n",
    "* Can read from a wide range of flat files, including Excel and HDF5\n",
    "* Can also read SQL queries into memory\n",
    "* Sensible defaults, automatically labeled and typed `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print a list of all read_* methods in pandas\n",
    "\n",
    "print(\"\".join([\"pd.%s\\n\" % reader \n",
    "               for reader in dir(pd) \n",
    "               if reader.startswith('read_')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DataFrame can be cast to files or other objects\n",
    "\n",
    "print(\"\".join([\"pd.DataFrame.%s\\n\" % reader \n",
    "               for reader in dir(pd.DataFrame) \n",
    "               if reader.startswith('to_')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Below are external links to details on the file formats that pandas can read.\n",
    "\n",
    "- [CSV](https://en.wikipedia.org/wiki/Comma-separated_values)\n",
    "- Excel\n",
    "- SQL\n",
    "- [JSON](http://www.json.org/)\n",
    "- [HDF5](https://www.hdfgroup.org/HDF5/)\n",
    "- [pickle](https://docs.python.org/3/library/pickle.html)\n",
    "- [msgpack](http://msgpack.org/)\n",
    "- [Stata](https://en.wikipedia.org/wiki/Stata)\n",
    "- [Google BigQuery](https://en.wikipedia.org/wiki/BigQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now demonstrate using pandas reading and writing to a variety of file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pandas.pydata.org/pandas-docs/stable/io.html#csv-text-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/exoplanets.csv', \n",
    "                 parse_dates=['DATE'],\n",
    "                 encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The first exoplanet to get news headlines was in 1996.\n",
    "# Use some indexing to inspect the rows.\n",
    "# Combining head() and .iloc[] bracket indexing.\n",
    "df.head(8).iloc[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some simple data extractions to prove we can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the name using [row index] and [column label]\n",
    "df.loc[7,'NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the mass using the [column label] and [row index]\n",
    "df.loc[7,'MASS(mjupiter)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract the mass using and [row index] and [column index]\n",
    "# The MASS(mjupiter) column is the 5th labeled column (not counting the Index)\n",
    "# All pandas DataFrames have the method .iloc() \n",
    "df.iloc[7,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.to_csv('tmp/exoplanets.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas can read a data table directly from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'http://waterdatafortexas.org/reservoirs/individual/austin.csv'\n",
    "df  = pd.read_csv(url, comment='#', index_col='date', parse_dates=True)\n",
    "df\n",
    "\n",
    "# note: you can parse dates, and ignore comment lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "http://pandas.pydata.org/pandas-docs/stable/io.html#excel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Save the DataFrame as an Excel file\n",
    "file_name = 'tmp/exoplanets.xls'\n",
    "clean_tmp(file_name)\n",
    "\n",
    "df.to_excel(file_name, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Read the Excel file\n",
    "file_name = 'tmp/exoplanets.xls'\n",
    "df = pd.read_excel(file_name, sheetnames=[0], encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pandas.pydata.org/pandas-docs/stable/io.html#sql-queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "file_name = \"tmp/exoplanets.sqlite\"\n",
    "clean_tmp(file_name)\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///' + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "df.to_sql('table', engine, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_sql('table', engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pandas.pydata.org/pandas-docs/stable/io.html#json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing to JSON file\n",
    "file_name = \"tmp/exoplanets.json\"\n",
    "clean_tmp(file_name)\n",
    "\n",
    "df.to_json(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# JSON is ASCII so we can inspect the file with the shell\n",
    "!head tmp/exoplanets.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading the JSON file results in columns sorted by label\n",
    "\n",
    "file_name = \"tmp/exoplanets.json\"\n",
    "df = pd.read_json(file_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_json?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## HDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://pandas.pydata.org/pandas-docs/stable/io.html#hdf5-pytables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas support 2 formats, ``fixed`` and ``table``. \n",
    "\n",
    "- ``fixed`` stores a heterogenous structure, but it can not be appended, nor queried, except via positional indexing (its much like a numpy record array).\n",
    "\n",
    "- ``table`` stores a heterogenous structure, very much like a table. It can be appending row-wise, and queried similarly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "file_name = \"tmp/exoplanets.hdf\"\n",
    "clean_tmp(file_name)\n",
    "\n",
    "df.to_hdf(file_name,\n",
    "          'df',\n",
    "           mode='w',\n",
    "           format='table',\n",
    "           encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "file_name = \"tmp/exoplanets.hdf\"\n",
    "df = pd.read_hdf(file_name,'df',encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## FAQ: File Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequently asked file-format questions, and answers:\n",
    "\n",
    "- which formats provide good fidelity\n",
    "  - hdf5, pickle, msgpack\n",
    "  \n",
    "- which formats can you query\n",
    "  - hdf5, sql\n",
    "  \n",
    "- which formats can you iterate\n",
    "  - csv, hdf5, sql\n",
    "  \n",
    "- which formats provide better interoprability\n",
    "  - csv, json, excel\n",
    "  \n",
    "- which formats can you transmit over the wire\n",
    "  - json, msgpack\n",
    "  \n",
    "- which formats have better compression\n",
    "  - hdf5, pickle, msgpack\n",
    "  \n",
    "- which formats allow multiple datasets in the same file\n",
    "  - hdf5, msgpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reading Large Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set some expectations for read times for various file formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Timing Reads: Common Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit pd.read_excel('tmp/exoplanets.xls', sheetnames=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit pd.read_sql('table', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit pd.read_json('tmp/exoplanets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit pd.read_csv('tmp/exoplanets.csv', parse_dates=['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit pd.read_hdf('tmp/exoplanets.hdf','df')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing Reads: Pickle and MSGPack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.to_pickle('tmp/exoplanets.pkl')\n",
    "df.to_msgpack('tmp/exoplanets.msgpack',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit pd.read_pickle('tmp/exoplanets.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%timeit pd.read_msgpack('tmp/exoplanets.msgpack', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Compressed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may encounter compressed data files, e.g. ZIP or TAR.\n",
    "\n",
    "Here we deomstrate some simple strategies for handling compressed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests, zipfile, io \n",
    "\n",
    "# Note: in python3 use io.BytesIO, in python2 use BytesIO.BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Compressed file from Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ``infer_datetime_format=True`` interprets the datetime format as an ISO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "# Just read from disk\n",
    "chunks = pd.read_csv('data/201509-citibike-tripdata.csv.gz', \n",
    "                      index_col=0,\n",
    "                      parse_dates=['starttime'],\n",
    "                      infer_datetime_format=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from URL and Extract to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "## Timing: about 15-180 seconds, depends on network, laptop \n",
    "\n",
    "# Get data, save to disk, read from disk\n",
    "u = 'https://s3.amazonaws.com/tripdata/201509-citibike-tripdata.zip'\n",
    "r = requests.get(u)\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "\n",
    "# Write to local disk, then read from local disk\n",
    "z.extractall(path='tmp')\n",
    "f = 'tmp/201509-citibike-tripdata.csv'\n",
    "df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from URL and Extract Without Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "## Timing: about 15-60 seconds, depending on network, laptop \n",
    "\n",
    "u  = 'https://s3.amazonaws.com/tripdata/201509-citibike-tripdata.zip'\n",
    "r  = requests.get(u)\n",
    "z  = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "f  = '201509-citibike-tripdata.csv' # name of file *INSIDE* the ZIP archive\n",
    "df = pd.read_csv(z.open(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "chunks = pd.read_csv('data/201509-citibike-tripdata.csv.gz', \n",
    "                      index_col=0,\n",
    "                      parse_dates=['starttime'],\n",
    "                      infer_datetime_format=True,\n",
    "                      chunksize=10000)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(\"%d -> %d\" % (i, len(chunk)))\n",
    "    # Note: for chunksize=10000, number of chunks is about 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Temporary Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: clean up tmp files created in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Always check for existence before removing\n",
    "if os.path.exists(\"tmp\"):\n",
    "    os.listdir(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a list of files inside, remove each one by one, then remove the dir\n",
    "if os.path.exists(\"tmp\"):\n",
    "    file_list = os.listdir(\"tmp\")\n",
    "    for file in file_list:\n",
    "        clean_tmp(os.path.join(\"tmp\", file))\n",
    "    os.rmdir(\"tmp\")\n",
    "    \n",
    "    \n",
    "# other examples depend on ./tmp, so we recreate it here\n",
    "os.mkdir('tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas Demomstration: Reading and Cleaning\n",
    "* reading a CSV file\n",
    "* column labels\n",
    "* adding and removing columns\n",
    "\n",
    "Pandas for Reading File Formats\n",
    "* Excel\n",
    "* HDF5\n",
    "* MySQL\n",
    "* JSON\n",
    "\n",
    "Pandas for Reading Large Data\n",
    "* timing read times for various formats\n",
    "* reading from compressed files\n",
    "* reading in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "continuum": {
   "depends": [
    "pd_intro",
    "pd_data_structs"
   ],
   "requires": [
    "data/exoplanets.csv",
    "data/201509-citibike-tripdata.csv.gz"
   ],
   "tag": "pd_data_io"
  },
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
