{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Learning Objectives](#Learning-Objectives)\n",
    "* [Importing our libraries](#Importing-our-libraries)\n",
    "\t* [Some Simple Data](#Some-Simple-Data)\n",
    "\t* [A Simple kNN Classifier](#A-Simple-kNN-Classifier)\n",
    "\t* [Simple Evaluation](#Simple-Evaluation)\n",
    "\t* [Visualization using two features](#Visualization-using-two-features)\n",
    "\t* [Exercise (exploring grid_step and number of neighbors)](#Exercise-%28exploring-grid_step-and-number-of-neighbors%29)\n",
    "* [Simple Comparison](#Simple-Comparison)\n",
    "* [Synthetic Datasets](#Synthetic-Datasets)\n",
    "\t* [make_blobs](#make_blobs)\n",
    "\t* [make_classification](#make_classification)\n",
    "* [Downloading Common Datasets](#Downloading-Common-Datasets)\n",
    "\t* [Exercise](#Exercise)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completion of this module, learners should be able to:\n",
    "\n",
    "* Explain what KNN classification and logistic regression are\n",
    "* Apply the KNN classifier\n",
    "* Develop training/testing sets and perform model validation.\n",
    "\n",
    "\n",
    "* Work with primary component analysis and support vector machines.\n",
    "* Compare optimization and curve fitting techniques.\n",
    "\n",
    "K-Nearest neighbor algorithms fall into regression and classification.  In classification, a K-nearest neighbor method uses local vote counts for class membership based on K nearest neighbors considered.  A K==1 model considers only the nearest neighbor.\n",
    "\n",
    "Logistic regression is fitting a logistic distribution to continuous data to model a binomial or multinomial response.  An example, described [here](https://en.wikipedia.org/wiki/Logistic_regression), is a logistic regression that predicts probability of success/failure on an exam given observations of passing/failing and the hours studied in advance.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import (cross_validation, datasets,\n",
    "                     decomposition,\n",
    "                     grid_search, linear_model, \n",
    "                     neighbors, metrics)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Simple Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "examples = iris.data\n",
    "classes  = iris.target\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the \"shape\" of the data\n",
    "df_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df_iris['species'] = iris.target\n",
    "df_iris['species_name'] = [iris.target_names[i] for i in iris.target]\n",
    "df_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a training and a testing set from this data by choosing indices\n",
    "# (wait a few cells for a better API)\n",
    "\n",
    "# Random order of indices\n",
    "n_examples = len(examples)\n",
    "shuffled_indices = np.random.permutation(n_examples)\n",
    "\n",
    "# Pick a trainig/testing split\n",
    "train_pct = 0.8\n",
    "train_ct  = int(n_examples * train_pct)\n",
    "\n",
    "# Select indices for training and testing\n",
    "train_idx, test_idx = shuffled_indices[:train_ct], shuffled_indices[train_ct:]\n",
    "train_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple kNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn5 = neighbors.KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn5.fit(examples[train_idx], classes[train_idx])\n",
    "predictions = knn5.predict(examples[test_idx])\n",
    "print(metrics.accuracy_score(predictions, classes[test_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization using two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets.make_classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the punch line is to predict for a large grid of data points\n",
    "# http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n",
    "def KNN_2D_map(twodim):\n",
    "    grid_step = 0.1\n",
    "    knn5 = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "    knn5.fit(twodim, classes)\n",
    "\n",
    "    # create testing data points on the standard \n",
    "    # Cartesian grid (over our data range)\n",
    "    # to color the background\n",
    "    maxes = np.max(twodim, axis=0) + 2*grid_step\n",
    "    mins  = np.min(twodim, axis=0) -   grid_step\n",
    "\n",
    "    xs,ys = np.mgrid[mins[0]:maxes[0]:grid_step, \n",
    "                     mins[1]:maxes[1]:grid_step]\n",
    "    grid_points = np.c_[xs.ravel(), ys.ravel()]\n",
    "\n",
    "    p = knn5.predict(grid_points)\n",
    "\n",
    "    # plot the predictions at the grid points\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.gca()\n",
    "    ax.pcolormesh(xs,ys,p.reshape(xs.shape))\n",
    "\n",
    "    ax.set_xlim(mins[0], maxes[0]-grid_step)\n",
    "    ax.set_ylim(mins[1], maxes[1]-grid_step)\n",
    "    \n",
    "twodim = examples[:,:2] # select first two features\n",
    "KNN_2D_map(twodim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twodim2 = examples[:,2:]  # choose different features\n",
    "KNN_2D_map(twodim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (exploring grid_step and number of neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick question: why did we add an extra `grid_step` value to the maxes, above?\n",
    "\n",
    "Investigate what happens to the decision boundary as we raise or lower the number of neighbors?  You could start answering this trying a range of neighbor values:  $k=3,5,10,15$.  Could the `grid_step` parameter mislead us, if we aren't paying close attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll learn about a more efficient comparison method in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn5 = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "logreg = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn5.fit(examples[train_idx], classes[train_idx])\n",
    "logreg.fit(examples[train_idx], classes[train_idx])\n",
    "\n",
    "lr_preds = logreg.predict(examples[test_idx])\n",
    "knn5_preds = knn5.predict(examples[test_idx])\n",
    "\n",
    "for preds in [lr_preds, knn5_preds]:\n",
    "    print(metrics.accuracy_score(preds, classes[test_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.datasets.make_blobs(n_samples=100, \n",
    "                             n_features=2,\n",
    "                             centers=3,         # number of classes\n",
    "                             cluster_std=1.0)   # shared -or- class-by-class`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, y = datasets.make_blobs(n_samples=50)\n",
    "plt.scatter(x[:,0], x[:,1], c=y, s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn.datasets.make_classification()`\n",
    "\n",
    "Many, many arguments.  See: http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\n",
    "\n",
    "For examples, see:  http://scikit-learn.org/stable/auto_examples/datasets/plot_random_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x,y = datasets.make_classification(n_features=2, n_redundant=0, n_informative=2,\n",
    "                                   n_clusters_per_class=1, n_classes=3)\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Common Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_dwn_1 = datasets.fetch_mldata('iris', data_home=\"./data\")\n",
    "print(iris_dwn_1.data.shape)\n",
    "print(iris_dwn_1.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_dwn_2 = datasets.fetch_mldata('datasets-UCI Iris',\n",
    "                                   target_name='class', \n",
    "                                   data_name='double0',\n",
    "                                   data_home=\"./data\")\n",
    "print(iris_dwn_2.data.shape)\n",
    "print(iris_dwn_2.target.shape)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "continuum": {
   "depends": [],
   "requires": [
    "data/wine.csv"
   ],
   "tag": "ml_knn"
  },
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
